
# Supervised Fine-tuning (SFT)

## Data Preparation

The data used in SFT should be in a JSON list. And each element of data should be a JSON dictionary including "id", "image", "conversations" at least. For instance, the data should in the format below:

```JSON
[
    {
        "id": ... ,
        "image": [name of the image file] or [null],
        "conversations": [
            {
                "from": "human",
                "value": [instruction or question]
            },
            {
                "from": "gpt",
                "value": [label]
            }
        ]
    },
    ... 
]
```

## Run with Shell

Run with the shell script shown below:

```Shell
#!/bin/bash

CUR_DIR=`pwd`

ROOT=${CUR_DIR}

export PYTHONPATH=${ROOT}:${PYTHONPATH}

deepspeed --include localhost:${DEVICE} --master_port 12345 training/sft_training/sft_main.py \
    ...(append your parameters here)
```
### Parameters:
* `data_path` - Path of the data used in this script, the data should be organized into the format listed above.
* `data_debug_path` - If provided, will save 10 training samples to the path for debugging purposes.
* `image_folder` - Path of the location where image files are stored. Make sure your image files can be accessed with the path (image_folder)/(name of the image).
* `data_train_split_ratio` - Ratio of the dataset to be split as train data. The remaining becomes eval data. Default: 0.9
* `is_sft_stage` - Decide whether to return sft loss. 
* `offload` - Enable ZeRO Offload techniques.
* `dataset_names` - Name of the dataset used in the script. When using this Python script, pass 'llava_reward' with this parameter.
* `dataset_samples` - Number of samples used in the dataset. Pass 'all' when using the whole dataset or just pass an integer of the number of samples when only using a part of the dataset. Default: 'all'
* `dataset_concatenate_samples` - Num of samples concatenated from each dataset. Default: 1
* `max_num_image_per_sample` - The maximum number of images per sample. Default: 8
* `per_device_train_batch_size` - Batch size (per device) for the training dataloader. Default: 2
* `per_device_eval_batch_size` - Batch size (per device) for the evaluation dataloader. Default: 2
* `max_seq_len` - Maximum length of the sequence generated by the LLM. Â The LLM will stop generating when the length of the generated sequence arrives at the value set by this parameter. Default: 4096
* `learning_rate` - Initial learning rate (after the potential warmup period) to use. Default: 1e-3
* `learning_rate_pretraining_components` - Initial learning rate for pre-trained weight, e.g., embedding (after the potential warmup period) to use. Default: 0
* `weight_decay` - Weight decay to use. Default: 0
* `num_train_epochs` - Total number of training epochs to perform. Default: 6
* `gradient_accumulation_steps` - Number of update steps to accumulate before performing a backward/update pass. Default: 1
* `lr_scheduler_type` - The scheduler type to use. Chose among "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup". Default: cosine
* `num_warmup_steps` - Number of steps (>1) or ratios (<=1) for the warmup in the lr scheduler. Default: 0
* `output_dir` - Where to store the model.
* `seed` - "A seed for reproducible training. Default: 1234
* `local_rank` - local_rank for distributed training on GPUs. Default: -1
* `gradient_checkpointing` - Enable HF gradient checkpointing for the model.
* `lm_model_name_or_path` - Path to the LLM base of the vision LLM. Make sure config files exist.
* `vision_model_name_or_path` - Path to the vision model base of the vision LLM.
* `enable_mmca_attention` - Add this parameter when any image is involved in the input.
* `vis_proj` - [baseline, vit, or perceiver], used to project vision feature to LLM embedding. Default: baseline
* `zero_stage` - ZeRO optimization stage for Actor model (and clones). Default: 0
* `precision` - Data precision used in this Python script. Pass 'fp16' or 'bf16' as you like. Default: fp16
* `enable_tensorboard` - Enable tensorboard logging
* `lang_lora_dim` - Use LoRA for fine-tuning language decoder (> 0). Default: 0
* `lang_lora_module_name` - The scope name of the target LoRA parameters. Default: model.layers.
* `vis_lora_dim` - Use LoRA for fine-tuning the visual encoder (> 0). Default: 0
* `vis_lora_module_name` - The scope name of the target LoRA parameters. Default: encoder.layers.
* `only_optimize_lora` - Only optimize the LoRA parameters.
* `template` - Prompt style of the input. Select the correct one according to the LLM base of the vision LLM. Default: default
* `eval_step` - The evaluation will be conducted for every specific number of training steps. Default: 100
* `from_checkpoint` - Specifying the checkpoint directory to be loaded.
* `vis_encoder_update` - Enable vision encoder update.
* `lang_decoder_update` - Enable LLM update.